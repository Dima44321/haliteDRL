{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.1 64-bit ('base': conda)",
   "display_name": "Python 3.7.1 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "43fb28534adcf5d5d1d5456b0506f47d1d45cab9d77e63353d0dd5a819ba8a55"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Resources:\n",
    "\n",
    "https://towardsdatascience.com/breaking-down-richard-suttons-policy-gradient-9768602cb63b\n",
    "\n",
    "http://karpathy.github.io/2016/05/31/rl/\n",
    "\n",
    "https://medium.com/@ts1829/policy-gradient-reinforcement-learning-in-pytorch-df1383ea0baf"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import board_funcs\n",
    "import models\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize ship and shipyard networks\n",
    "s_net = models.ShipNet().to(device)\n",
    "sy_net = models.ShipYardNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#toy dataset with action-reward pairs\n",
    "#reward is your halite - enemy halite  \n",
    "#assumes one-hot encoding of actions\n",
    "#TODO: Implement experiance replay via rollouts\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "ship_data = (torch.rand((batch_size, models.ship_action_space_size)), torch.rand(batch_size))\n",
    "ship_data = (torch.rand((batch_size, models.shipyard_action_space_size)), torch.rand(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init optimizer \n",
    "lr = 0.002\n",
    "\n",
    "opt_s = torch.optim.Adam(s_net.parameters(), lr = lr)\n",
    "opt_sy = torch.optim.Adam(sy_net.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function \n",
    "#TODO: reward history and discounted reward\n",
    "def loss(action, reward):\n",
    "    log_p = torch.log(action)\n",
    "    return reward*log_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for j in range(len(ship_data)):\n",
    "\n",
    "        #get action-reward pairs\n",
    "        s_action = ship_data[0]\n",
    "        sy_action = sy_data[0]\n",
    "\n",
    "        s_reward = ship_data[1]\n",
    "        sy_reward = ship_data[1]\n",
    "\n",
    "        #calc losses\n",
    "        loss_s = loss(s_action, s_reward)\n",
    "        loss_sy = loss(sy_action, sy_reward)\n",
    "\n",
    "        #backpropogate and update weights\n",
    "        loss_s.backward()\n",
    "        loss_sy.backward()\n",
    "        opt_s.step()\n",
    "        opt_sy.step()\n",
    "\n",
    "        #zero gradients for next pass\n",
    "        s_net.zero_grad()\n",
    "        sy_net.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: self play via policy gradients"
   ]
  }
 ]
}